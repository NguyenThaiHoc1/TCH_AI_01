{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11c24545",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# import package\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "112f5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = os.path.dirname(os.path.abspath('')) # get path root folder\n",
    "path_dataset = os.path.join(path_root, 'dataset', 'dataset.xlsx')\n",
    "path_result_file = os.path.join(path_root, 'result', 'customer')\n",
    "file_save_result = os.path.join(path_result_file, 'result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3327af08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sensor_data(path):\n",
    "    return pd.read_excel(path, skiprows = range(0, 2))\n",
    "\n",
    "def drop_column(df):\n",
    "    x = [0, 1, 2, 3, 4] #  0: Unnamed-0, Unnamed-1, Unnamed-2,  データＩＤ, Time\n",
    "    df_droped = df.drop(df.columns[x], axis=1)\n",
    "    return df_droped\n",
    "\n",
    "\n",
    "# Task 1: Loading data from file excel \n",
    "data = load_sensor_data(path=path_dataset)\n",
    "\n",
    "# Task 2: change name column\n",
    "data_copy = data.copy()\n",
    "data_copy.rename(columns={'Unnamed: 0': 'datecheck-ID', \n",
    "                          'Unnamed: 1': 'shoes-ID',\n",
    "                          'Unnamed: 2': 'balance-status',\n",
    "                          'データＩＤ':'set-ID',\n",
    "                         }, inplace = True)\n",
    "\n",
    "# Task 3: removing feature which relate to train\n",
    "data_copy = drop_column(df=data_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1c1287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function from customer ... \n",
    "# def label_Integerization(label):\n",
    "#     uniq_label = np.unique(label)\n",
    "#     int_label=[]\n",
    "#     for i in range(len(label)):\n",
    "#         c_nn = label[i]\n",
    "#         for j in range(len(uniq_label)):\n",
    "#             if c_nn == uniq_label[j]:\n",
    "#                 int_label.append((j+1))\n",
    "#                 break\n",
    "#     return np.array(int_label)\n",
    "\n",
    "# Cái này không cần thiết\n",
    "# X = np.array(y_train.reshape(-1,1))\n",
    "# enc = OneHotEncoder(categories=\"auto\", sparse=False, dtype=np.int32)\n",
    "# y_train_xx = enc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25a1ff2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape is:  (236388, 48)\n",
      "Y Shape is:  (236388, 6)\n"
     ]
    }
   ],
   "source": [
    "selcol = [\"L-Fx1\",\"L-Fy1\",\"L-Fz1\",\"L-Mx1\",\"L-My1\",\"L-Mz1\",\"L-Fx2\",\"L-Fy2\",\"L-Fz2\",\"L-Mx2\",\"L-My2\",\"L-Mz2\",\"L-Fx3\",\"L-Fy3\",\"L-Fz3\",\"L-Mx3\",\"L-My3\",\"L-Mz3\",\n",
    "             \"L-AccelX\",\"L-AccelY\",\"L-AccelZ\",\"L-GyroX\",\"L-GyroY\",\"L-GyroZ\",\"R-Fx1\",\"R-Fy1\",\"R-Fz1\",\"R-Mx1\",\"R-My1\",\"R-Mz1\",\"R-Fx2\",\"R-Fy2\",\"R-Fz2\",\"R-Mx2\",\"R-My2\",\"R-Mz2\",\"R-Fx3\",\"R-Fy3\",\n",
    "             \"R-Fz3\",\"R-Mx3\",\"R-My3\",\"R-Mz3\",\"R-AccelX\",\"R-AccelY\",\"R-AccelZ\",\"R-GyroX\",\"R-GyroY\",\"R-GyroZ\"]\n",
    "    \n",
    "lblcol = [\"L-FX\",\"L-FY\",\"L-FZ\",\"R-FX\",\"R-FY\",\"R-FZ\"]\n",
    "\n",
    "x_train = data_copy[selcol].values # input \n",
    "y_train = data_copy[lblcol].values # labels <--- \n",
    "\n",
    "print(\"X Shape is: \", x_train.shape)\n",
    "print(\"Y Shape is: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17bf4c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self,logs={}):\n",
    "        self.losses = []\n",
    "    def on_batch_end(self,batch,logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "def set_callback(name_type):\n",
    "    if name_type == 'history':\n",
    "        return LossHistory\n",
    "    elif 'tensorboard' in name_type:\n",
    "        check_folds = name_type.split('_')\n",
    "        if len(check_folds) > 1:\n",
    "            numberfolds = check_folds[1]\n",
    "            logs = os.path.join(path_result_file, \"folds\", \"tensorboard_%s\" % (str(numberfolds)))\n",
    "            return tf.keras.callbacks.TensorBoard(log_dir=logs, \n",
    "                                                  histogram_freq=1, \n",
    "                                                  write_graph=True,  \n",
    "                                                  write_images=False)\n",
    "            \n",
    "        else:\n",
    "            logs = os.path.join(path_result_file, \"tensorboard\")\n",
    "            return tf.keras.callbacks.TensorBoard(log_dir=logs, \n",
    "                                                  histogram_freq=1, \n",
    "                                                  write_graph=True,  \n",
    "                                                  write_images=False)\n",
    "    \n",
    "    elif 'checkpoint' in name_type:\n",
    "        check_folds = name_type.split('_')\n",
    "        if len(check_folds) > 1:\n",
    "            numberfolds = check_folds[1]\n",
    "            saving_path = os.path.join(path_result_file, \"checkpoint\", \"weights_%s_{epoch:02d}_{accuracy:.2f}.hdf5\" % (str(numberfolds)))\n",
    "            return tf.keras.callbacks.ModelCheckpoint(saving_path, \n",
    "                                                      verbose=1, \n",
    "                                                      monitor=\"loss\",\n",
    "                                                      save_best_only=True,\n",
    "                                                      mode='auto') # save_best_only=True,\n",
    "            \n",
    "        else:\n",
    "            saving_path = os.path.join(path_result_file, \"checkpoint\", \"weights_{epoch:02d}_{accuracy:.2f}.hdf5\")\n",
    "            return tf.keras.callbacks.ModelCheckpoint(saving_path, \n",
    "                                                      verbose=1, \n",
    "                                                      monitor=\"loss\",\n",
    "                                                      save_best_only=True,\n",
    "                                                      mode='auto') # save_best_only=True, \n",
    "\n",
    "\n",
    "class DNN(object):\n",
    "    def __init__(self, n_input, n_hiddens, n_out, activation='relu',\n",
    "                 kernel_initializer='he_normal', optimize='Adam', param={'rate': 0.001, \"beta1\": 0.9, \"beta2\": 0.999},\n",
    "                 loss='categorical_crossentropy', on_softmax=True):\n",
    "\n",
    "        self.n_in = n_input\n",
    "        self.n_hiddens = n_hiddens\n",
    "        self.n_out = n_out\n",
    "        self.activation = activation\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.optimize = self.set_optimizer(optimize, param)\n",
    "        self.loss = loss\n",
    "        self.on_softmax = on_softmax\n",
    "        self.callback = None\n",
    "        self.hist = None\n",
    "\n",
    "    def set_optimizer(self, optimize, param):\n",
    "        if optimize == 'Adam':\n",
    "            learning_rate = param['rate']\n",
    "            beta1 = param['beta1']\n",
    "            beta2 = param['beta2']\n",
    "            optimizer = optimizers.Adam(lr=learning_rate, beta_1=beta1, beta_2=beta2)\n",
    "        elif optimize == 'SGD':\n",
    "            lr = prm['lr']\n",
    "            momentum = param['mometum']\n",
    "            self.optimizer = optimizers.SGD(lr, momentum)\n",
    "        elif optimize == 'Nest':\n",
    "            lr = param['lr']\n",
    "            momentum = param['mometum']\n",
    "            optimizer = optimizers.SGD(lr, momentum, nesterov=True)\n",
    "        elif optimize == 'Adagrad':\n",
    "            lr = param['lr']\n",
    "            optimizer = optimizers.Adagrad(lr)\n",
    "        elif optimize == 'Adadelta':\n",
    "            rho = param['rho']\n",
    "            optimizer = optimizers.Adadelta(rho=rho)\n",
    "        elif optimize == 'RMSProp':\n",
    "            lr = param['lr']\n",
    "            optimizer = optimizers.RMSprop(lr)\n",
    "        else:\n",
    "            learning_rate = param['rate']\n",
    "            beta1 = param['beta1']\n",
    "            beta2 = param['beta2']\n",
    "            optimizer = optimizers.Adam(lr=learning_rate, beta_1=beta1, beta_2=beta2)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def make_model_bach_normalization(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.n_hiddens[0], kernel_initializer=self.kernel_initializer,\n",
    "                        input_shape=(self.n_in,)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(self.activation))\n",
    "        model.add(Dropout(0.5))\n",
    "        for i in range(1, len(self.n_hiddens)):\n",
    "            model.add(Dense(self.n_hiddens[i], kernel_initializer=self.kernel_initializer))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Activation(self.activation))\n",
    "            model.add(Dropout(0.5))\n",
    "        if self.on_softmax:\n",
    "            model.add(Dense(self.n_out, kernel_initializer=self.kernel_initializer,\n",
    "                            activation='softmax'))\n",
    "        else:\n",
    "            model.add(Dense(self.n_out))\n",
    "\n",
    "        model.summary()\n",
    "        self.model = model\n",
    "        self.model.compile(optimizer=self.optimize,\n",
    "                           loss=self.loss,\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def make_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.n_hiddens[0], kernel_initializer=self.kernel_initializer,\n",
    "                        input_shape=(self.n_in,)))\n",
    "        model.add(Activation(self.activation))\n",
    "        model.add(Dropout(0.5))\n",
    "        for i in range(1, len(self.n_hiddens)):\n",
    "            model.add(Dense(self.n_hiddens[i], kernel_initializer=self.kernel_initializer))\n",
    "            model.add(Activation(self.activation))\n",
    "            model.add(Dropout(0.5))\n",
    "        if self.on_softmax:\n",
    "            model.add(Dense(self.n_out, kernel_initializer=self.kernel_initializer,\n",
    "                            activation='softmax'))\n",
    "        else:\n",
    "            model.add(Dense(self.n_out))\n",
    "\n",
    "        model.summary()\n",
    "        self.model = model\n",
    "        self.model.compile(optimizer=self.optimize,\n",
    "                           loss=self.loss,\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def set_callback(self, callback_type):\n",
    "        list_callback = []\n",
    "        for name_callback in callback_type:\n",
    "            callback = set_callback(name_type=name_callback)\n",
    "            list_callback.append(callback)\n",
    "        self.callback = list_callback        \n",
    "\n",
    "    def calc_fit(self, x_data, y_data, epoch=10,\n",
    "                 batch_size=5, verbose=2, validation=None):\n",
    "        print(\">>> Calc fit shape is : %s  -  %s\" % (x_data.shape, y_data.shape))\n",
    "        if validation is None:\n",
    "            if self.callback is None:\n",
    "                self.hist = self.model.fit(x_data, y_data, epochs=epoch,\n",
    "                                           batch_size=batch_size, \n",
    "                                           verbose=verbose)\n",
    "            else:\n",
    "                self.hist = self.model.fit(x_data, y_data, epochs=epoch,\n",
    "                                           batch_size=batch_size, \n",
    "                                           verbose=verbose, \n",
    "                                           callbacks=self.callback)\n",
    "        else:\n",
    "            if self.callback is None:\n",
    "                self.hist = self.model.fit(x_data, y_data, epochs=epoch,\n",
    "                                           batch_size=bach_size, \n",
    "                                           verbose=verbose, \n",
    "                                           validation=validation)\n",
    "            else:\n",
    "                self.hist = self.model.fit(x_data, y_data, epochs=epoch,\n",
    "                                           batch_size=bach_size, \n",
    "                                           verbose=verbose, \n",
    "                                           validation=validation,\n",
    "                                           callbacks=self.callback)\n",
    "\n",
    "    def get_history_data(self):\n",
    "        return self.hist\n",
    "\n",
    "    def evaluate(self, x_test, y_test, varbose=0):\n",
    "        loss, acc = self.model.evaluate(x_test, ytest, verbose=varbose)\n",
    "        return loss, acc\n",
    "\n",
    "    def predict(self, x_test, varbose=0):\n",
    "        pred = self.model.predict(x_test)\n",
    "        return pred\n",
    "\n",
    "    def save_model(self, outf):\n",
    "        self.model.save(outf)  # モデルを保存\n",
    "\n",
    "    def load_model(self, inpf):\n",
    "        self.model = None\n",
    "        self.model = load_model(inpf)\n",
    "        self.model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f1bfeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_1(x_train, y_train, file_save):\n",
    "    n_inp = 48\n",
    "    n_hidden = [150, 50, 10]\n",
    "    n_out = 6 # n_out = 1 - is wrong about when MSE calculate the loss \n",
    "    epoch = 1000\n",
    "    \n",
    "    # setting training \n",
    "    dnn = DNN(n_inp, n_hidden, n_out, loss='mse', optimize='RMSProp', param={'lr':0.001}, on_softmax=False)\n",
    "    dnn.make_model_bach_normalization()\n",
    "    dnn.set_callback(callback_type=['tensorboard', 'checkpoint'])\n",
    "    dnn.calc_fit(x_train, y_train, batch_size=30, epoch=epoch)\n",
    "    \n",
    "    # saving model\n",
    "    \n",
    "    \n",
    "    # write result\n",
    "    hist = dnn.get_history_data()\n",
    "    list_d = []\n",
    "    step = [x + 1 for x in range(epoch)]\n",
    "    acc = hist.history['accuracy']\n",
    "    loss = hist.history['loss']\n",
    "    for i in range(len(step)):\n",
    "        c_d = [step[i], acc[i], loss[i]]\n",
    "        list_d.append(c_d)\n",
    "    \n",
    "    # write file csv \n",
    "    d_hist = pd.DataFrame(list_d, columns=['step','accuracy','loss'])\n",
    "    d_hist.to_csv(file_save, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f6fa517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold \n",
    "\n",
    "def main_crossvalidation(x_train, y_train, file_save):\n",
    "    # get info\n",
    "    basename = os.path.basename(file_save_result)\n",
    "    dirname = os.path.dirname(file_save_result)\n",
    "    \n",
    "    # parameter for training \n",
    "    n_inp = 48\n",
    "    n_hidden = [150, 50, 10]\n",
    "    n_out = 6 # n_out = 1 - is wrong about when MSE calculate the loss \n",
    "    epoch = 500\n",
    "    \n",
    "    # init cross validation\n",
    "    from sklearn.model_selection import KFold\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for index, (train_index, test_index) in enumerate(kfold.split(x_train, y_train)):\n",
    "        print(\">>> FOLD is %s\" % (str(index)))\n",
    "        file_save = str(basename.split('.')[0] + '_%s' % (index + 1)) + str(basename.split('.')[1])\n",
    "        abs_path = os.path.join(dirname, file_save)\n",
    "        \n",
    "        X_train_folds = x_train[train_index]\n",
    "        y_train_folds = y_train[train_index]\n",
    "        \n",
    "        X_test_fold = x_train[test_index]\n",
    "        y_test_fold = y_train[test_index]\n",
    "        \n",
    "        # Clone model \n",
    "        dnn = DNN(n_inp, n_hidden, n_out, loss='mse', optimize='RMSProp', param={'lr':0.001}, on_softmax=False)\n",
    "        dnn.make_model_bach_normalization()\n",
    "        dnn.set_callback(callback_type=['tensorboard_%s' % (index + 1), 'checkpoint_%s' % (index + 1)])\n",
    "        dnn.calc_fit(X_train_folds, y_train_folds, \n",
    "                     batch_size=128, \n",
    "                     epoch=epoch, validation=(X_test_fold, y_test_fold)) # FJN: 128\n",
    "        \n",
    "        \n",
    "        # write result\n",
    "        hist = dnn.get_history_data()\n",
    "        list_d = []\n",
    "        step = [x + 1 for x in range(epoch)]\n",
    "        acc = hist.history['accuracy']\n",
    "        loss = hist.history['loss']\n",
    "        for i in range(len(step)):\n",
    "            c_d = [step[i], acc[i], loss[i]]\n",
    "            list_d.append(c_d)\n",
    "\n",
    "        # write file csv \n",
    "        d_hist = pd.DataFrame(list_d, columns=['step','accuracy','loss'])\n",
    "        d_hist.to_csv(abs_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4e63b9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Training Data: ((236388, 48), (236388, 6))\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape Training Data: {x_train.shape, y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9dc7e9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-03 12:50:06.102560: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 150)               7350      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 150)              600       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 150)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 150)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                7550      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 50)               200       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 50)                0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                510       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 10)               40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,316\n",
      "Trainable params: 15,896\n",
      "Non-trainable params: 420\n",
      "_________________________________________________________________\n",
      ">>> Calc fit shape is : (236388, 48)  -  (236388, 6)\n",
      "Epoch 1/500\n",
      "\n",
      "Epoch 00001: loss improved from inf to 54306.74219, saving model to /Users/thaihoc/Desktop/TCH_AI/result/customer/checkpoint/weights_1_01_0.58.hdf5\n",
      "1847/1847 - 13s - loss: 54306.7422 - accuracy: 0.5776 - 13s/epoch - 7ms/step\n",
      "Epoch 2/500\n",
      "\n",
      "Epoch 00002: loss improved from 54306.74219 to 39638.52344, saving model to /Users/thaihoc/Desktop/TCH_AI/result/customer/checkpoint/weights_1_02_0.66.hdf5\n",
      "1847/1847 - 8s - loss: 39638.5234 - accuracy: 0.6557 - 8s/epoch - 4ms/step\n",
      "Epoch 3/500\n",
      "\n",
      "Epoch 00003: loss improved from 39638.52344 to 24950.42383, saving model to /Users/thaihoc/Desktop/TCH_AI/result/customer/checkpoint/weights_1_03_0.67.hdf5\n",
      "1847/1847 - 11s - loss: 24950.4238 - accuracy: 0.6674 - 11s/epoch - 6ms/step\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 00004: loss improved from 24950.42383 to 17109.70703, saving model to /Users/thaihoc/Desktop/TCH_AI/result/customer/checkpoint/weights_1_04_0.72.hdf5\n",
      "1847/1847 - 8s - loss: 17109.7070 - accuracy: 0.7162 - 8s/epoch - 4ms/step\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 00005: loss improved from 17109.70703 to 15044.83105, saving model to /Users/thaihoc/Desktop/TCH_AI/result/customer/checkpoint/weights_1_05_0.73.hdf5\n",
      "1847/1847 - 9s - loss: 15044.8311 - accuracy: 0.7269 - 9s/epoch - 5ms/step\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 00006: loss improved from 15044.83105 to 14662.70215, saving model to /Users/thaihoc/Desktop/TCH_AI/result/customer/checkpoint/weights_1_06_0.73.hdf5\n",
      "1847/1847 - 9s - loss: 14662.7021 - accuracy: 0.7295 - 9s/epoch - 5ms/step\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 00007: loss improved from 14662.70215 to 14485.98340, saving model to /Users/thaihoc/Desktop/TCH_AI/result/customer/checkpoint/weights_1_07_0.73.hdf5\n",
      "1847/1847 - 8s - loss: 14485.9834 - accuracy: 0.7318 - 8s/epoch - 4ms/step\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 00008: loss improved from 14485.98340 to 14245.25684, saving model to /Users/thaihoc/Desktop/TCH_AI/result/customer/checkpoint/weights_1_08_0.73.hdf5\n",
      "1847/1847 - 8s - loss: 14245.2568 - accuracy: 0.7337 - 8s/epoch - 4ms/step\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 00009: loss improved from 14245.25684 to 14094.98633, saving model to /Users/thaihoc/Desktop/TCH_AI/result/customer/checkpoint/weights_1_09_0.73.hdf5\n",
      "1847/1847 - 7s - loss: 14094.9863 - accuracy: 0.7337 - 7s/epoch - 4ms/step\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 00010: loss improved from 14094.98633 to 13972.33301, saving model to /Users/thaihoc/Desktop/TCH_AI/result/customer/checkpoint/weights_1_10_0.74.hdf5\n",
      "1847/1847 - 7s - loss: 13972.3330 - accuracy: 0.7366 - 7s/epoch - 4ms/step\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 00011: loss improved from 13972.33301 to 13876.10938, saving model to /Users/thaihoc/Desktop/TCH_AI/result/customer/checkpoint/weights_1_11_0.74.hdf5\n",
      "1847/1847 - 7s - loss: 13876.1094 - accuracy: 0.7365 - 7s/epoch - 4ms/step\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 00012: loss improved from 13876.10938 to 13786.01465, saving model to /Users/thaihoc/Desktop/TCH_AI/result/customer/checkpoint/weights_1_12_0.74.hdf5\n",
      "1847/1847 - 8s - loss: 13786.0146 - accuracy: 0.7363 - 8s/epoch - 5ms/step\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 00013: loss improved from 13786.01465 to 13686.47656, saving model to /Users/thaihoc/Desktop/TCH_AI/result/customer/checkpoint/weights_1_13_0.74.hdf5\n",
      "1847/1847 - 12s - loss: 13686.4766 - accuracy: 0.7376 - 12s/epoch - 6ms/step\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 00014: loss improved from 13686.47656 to 13632.87305, saving model to /Users/thaihoc/Desktop/TCH_AI/result/customer/checkpoint/weights_1_14_0.74.hdf5\n",
      "1847/1847 - 10s - loss: 13632.8730 - accuracy: 0.7385 - 10s/epoch - 6ms/step\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 00015: loss improved from 13632.87305 to 13509.29492, saving model to /Users/thaihoc/Desktop/TCH_AI/result/customer/checkpoint/weights_1_15_0.74.hdf5\n",
      "1847/1847 - 8s - loss: 13509.2949 - accuracy: 0.7383 - 8s/epoch - 5ms/step\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 00016: loss improved from 13509.29492 to 13470.66211, saving model to /Users/thaihoc/Desktop/TCH_AI/result/customer/checkpoint/weights_1_16_0.74.hdf5\n",
      "1847/1847 - 7s - loss: 13470.6621 - accuracy: 0.7394 - 7s/epoch - 4ms/step\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 00017: loss improved from 13470.66211 to 13367.02832, saving model to /Users/thaihoc/Desktop/TCH_AI/result/customer/checkpoint/weights_1_17_0.74.hdf5\n",
      "1847/1847 - 7s - loss: 13367.0283 - accuracy: 0.7402 - 7s/epoch - 4ms/step\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 00018: loss improved from 13367.02832 to 13289.82227, saving model to /Users/thaihoc/Desktop/TCH_AI/result/customer/checkpoint/weights_1_18_0.74.hdf5\n",
      "1847/1847 - 7s - loss: 13289.8223 - accuracy: 0.7395 - 7s/epoch - 4ms/step\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 00019: loss improved from 13289.82227 to 13273.04004, saving model to /Users/thaihoc/Desktop/TCH_AI/result/customer/checkpoint/weights_1_19_0.74.hdf5\n",
      "1847/1847 - 9s - loss: 13273.0400 - accuracy: 0.7399 - 9s/epoch - 5ms/step\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 00020: loss improved from 13273.04004 to 13197.79590, saving model to /Users/thaihoc/Desktop/TCH_AI/result/customer/checkpoint/weights_1_20_0.74.hdf5\n",
      "1847/1847 - 8s - loss: 13197.7959 - accuracy: 0.7408 - 8s/epoch - 4ms/step\n",
      "Epoch 21/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jy/5zjml4q54dv2fmdqz2xvwc3w0000gn/T/ipykernel_796/501221763.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_save_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_result_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'result.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain_crossvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_save_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/jy/5zjml4q54dv2fmdqz2xvwc3w0000gn/T/ipykernel_796/2134489149.py\u001b[0m in \u001b[0;36mmain_crossvalidation\u001b[0;34m(x_train, y_train, file_save)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_model_bach_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tensorboard_%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkpoint_%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# FJN: 128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/jy/5zjml4q54dv2fmdqz2xvwc3w0000gn/T/ipykernel_796/407120677.py\u001b[0m in \u001b[0;36mcalc_fit\u001b[0;34m(self, x_data, y_data, epoch, batch_size, verbose, validation)\u001b[0m\n\u001b[1;32m    161\u001b[0m                                            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                                            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                                            callbacks=self.callback)\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TCH_AI-WsmgR_L1/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TCH_AI-WsmgR_L1/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TCH_AI-WsmgR_L1/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TCH_AI-WsmgR_L1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TCH_AI-WsmgR_L1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TCH_AI-WsmgR_L1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TCH_AI-WsmgR_L1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TCH_AI-WsmgR_L1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TCH_AI-WsmgR_L1/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_save_result = os.path.join(path_result_file, 'result.csv')\n",
    "main_crossvalidation(x_train, y_train, file_save_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3b366068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> RMSE:  48.85347\n"
     ]
    }
   ],
   "source": [
    "# predict model \n",
    "model = tf.keras.models.load_model(\"/Users/thaihoc/Desktop/TCH_AI/result/customer/checkpoint/weights_05_0.73.hdf5\")\n",
    "\n",
    "# rmse \n",
    "x_predicted = model.predict(x_train[:5])\n",
    "x_labeled = y_train[:5]\n",
    "mse = tf.reduce_mean(tf.square(tf.subtract(x_predicted, x_labeled)))\n",
    "rmse = tf.sqrt(mse)\n",
    "print(\">> RMSE: \", rmse.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c652a06",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Supported target types are: ('binary', 'multiclass'). Got 'continuous-multioutput' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jy/5zjml4q54dv2fmdqz2xvwc3w0000gn/T/ipykernel_796/1807970228.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkFold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TCH_AI-WsmgR_L1/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    338\u001b[0m             )\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TCH_AI-WsmgR_L1/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_test_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mtrain_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mtest_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TCH_AI-WsmgR_L1/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_iter_test_masks\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_iter_test_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0mtest_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_folds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtest_folds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TCH_AI-WsmgR_L1/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_make_test_folds\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    652\u001b[0m             raise ValueError(\n\u001b[1;32m    653\u001b[0m                 \"Supported target types are: {}. Got {!r} instead.\".format(\n\u001b[0;32m--> 654\u001b[0;31m                     \u001b[0mallowed_target_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_of_target_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m                 )\n\u001b[1;32m    656\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Supported target types are: ('binary', 'multiclass'). Got 'continuous-multioutput' instead."
     ]
    }
   ],
   "source": [
    "# Cross validation \n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "kf = KFold(n_splits = 5)\n",
    "skf = StratifiedKFold(n_splits = 5, random_state = 7, shuffle = True) \n",
    "kFold = StratifiedKFold(n_splits=10)\n",
    "\n",
    "\n",
    "for train_index, val_index in kFold.split(x_train,y_train):\n",
    "    training_data = x_train[train_index]\n",
    "    validation_data = x_train[val_index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae531cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   5.56862745,    4.54901961,    2.82352941, ...,    0.91      ,\n",
       "          -3.08      ,   -3.29      ],\n",
       "       [  -4.43137255,   -1.45098039,    2.82352941, ...,    0.91      ,\n",
       "          -3.01      ,   -3.29      ],\n",
       "       [   3.56862745,   -3.45098039,    0.82352941, ...,    1.19      ,\n",
       "          -3.08      ,   -3.43      ],\n",
       "       ...,\n",
       "       [ 166.06122449,  190.40816327,  556.08163265, ..., -282.1       ,\n",
       "        -182.3       ,   -4.6       ],\n",
       "       [ 193.06122449,  197.40816327,  624.08163265, ..., -281.6       ,\n",
       "        -188.7       ,   14.8       ],\n",
       "       [ 219.06122449,  209.40816327,  696.08163265, ..., -270.5       ,\n",
       "        -190.4       ,   16.6       ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a672d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a258a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a3a9af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_check = np.asarray([[0.5, 0.3, 0.6, 0.1111, 0, 9]])\n",
    "y_predict = np.asarray([0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e51d068e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ed700965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.31622776601683794>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sqrt(tf.reduce_sum(tf.pow(tf.subtract(y_check, y_predict), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a220bc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.049999999999999996>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oss = tf.reduce_sum(tf.pow(y_check - y_predict, 2)) / (2 * 1)\n",
    "oss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "24f1c7d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y_true and y_pred have different number of output (1!=3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jy/5zjml4q54dv2fmdqz2xvwc3w0000gn/T/ipykernel_4443/2995808047.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/TCH_AI-WsmgR_L1/lib/python3.7/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \"\"\"\n\u001b[1;32m    423\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m     )\n\u001b[1;32m    426\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TCH_AI-WsmgR_L1/lib/python3.7/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m         raise ValueError(\n\u001b[1;32m    101\u001b[0m             \"y_true and y_pred have different number of output ({0}!={1})\".format(\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             )\n\u001b[1;32m    104\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: y_true and y_pred have different number of output (1!=3)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_check, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30adc589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mse = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "26a05cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.03333333507180214>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y_predict, y_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7655ab64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=4>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e75b6f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.876503868333335"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((y_predict-y_check)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6cd0151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float64, numpy=array([10.17986046])>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(tf.square(y_check - y_predict), axis=-1) / 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "956dfb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 19:16:31.599692: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.03333333333333333>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(tf.pow(y_check - y_predict, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1797dd2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float64, numpy=array([11.87650387])>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(tf.square(y_predict - y_check), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03540896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
